{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import holidays\n",
        "\n",
        "# Load datasets\n",
        "all_listings = pd.read_csv(\"/content/drive/MyDrive/merged_data/all_listings.csv\")\n",
        "all_calendars = pd.read_csv(\"/content/drive/MyDrive/merged_data/all_calendars.csv\")"
      ],
      "metadata": {
        "id": "jr0yjKzfyw6G"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# 1. Data Cleaning for Listings (Enhanced)\n",
        "# ==============================================\n",
        "def clean_listings(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean and engineer features for the listings dataset.\"\"\"\n",
        "    # Fix neighborhood names manually\n",
        "    def fix_neighborhood_manual(neighborhood: str) -> str:\n",
        "        if pd.isna(neighborhood):\n",
        "            return neighborhood\n",
        "        fixes = {\n",
        "            \"Vieux-Qu√©bec/Cap-Blanc/Colline parlementaire\": \"Vieux-Québec/Cap-Blanc/Colline parlementaire\",\n",
        "            \"Qu√©bec\": \"Québec\",\n",
        "        }\n",
        "        return fixes.get(neighborhood, neighborhood)\n",
        "\n",
        "    df['neighbourhood_cleansed'] = df['neighbourhood_cleansed'].apply(fix_neighborhood_manual)\n",
        "\n",
        "    # 1. Description handling\n",
        "    df['has_description'] = df['description'].notna().astype(int)\n",
        "\n",
        "    # 2. Host response time - preserve original categories\n",
        "    df['host_response_time'] = df['host_response_time'].fillna('no_response')\n",
        "\n",
        "    # 3. Superhost conversion with validation\n",
        "    df['host_is_superhost'] = df['host_is_superhost'].map({'t': 1, 'f': 0, None: 0}).fillna(0)\n",
        "\n",
        "    # 4. Bathroom features with enhanced pattern matching\n",
        "    bath_pattern = r'shared|share|communal|joint'\n",
        "    df['shared_bath'] = df['bathrooms_text'].str.lower().str.contains(bath_pattern, na=False).astype(int)\n",
        "\n",
        "    # 5. Numeric columns handling with improved outlier detection\n",
        "    numeric_cols = ['accommodates', 'bathrooms', 'bedrooms', 'beds']\n",
        "    for col in numeric_cols:\n",
        "        # Calculate median before handling outliers\n",
        "        col_median = df[col].median()\n",
        "        # Fill NA with median\n",
        "        df[col] = df[col].fillna(col_median)\n",
        "        # Clip values between 1st and 99th percentiles\n",
        "        lower = df[col].quantile(0.01)\n",
        "        upper = df[col].quantile(0.99)\n",
        "        df[col] = df[col].clip(lower, upper)\n",
        "        # Ensure non-negative values\n",
        "        df[col] = df[col].clip(lower=0)\n",
        "\n",
        "    # 6. Enhanced logical constraints\n",
        "    df['bedrooms'] = np.where(df['bedrooms'] > df['accommodates'], df['accommodates'], df['bedrooms'])\n",
        "    df['bathrooms'] = np.where(df['bathrooms'] > df['bedrooms'], df['bedrooms'], df['bathrooms'])\n",
        "    df['beds'] = np.where(df['beds'] > df['bathrooms']*2, df['bathrooms']*2, df['beds'])  # Assume max 2 beds per bathroom\n",
        "\n",
        "    # 7. Advanced amenities processing\n",
        "    amenities = df['amenities'].str.lower()\n",
        "    df['parking'] = amenities.str.contains(r'\\bparking\\b', regex=True).astype(int)\n",
        "    df['air_conditioning'] = amenities.str.contains(r'air conditioning| a/c\\b|ac unit', regex=True).astype(int)\n",
        "    df['wifi'] = amenities.str.contains(r'wifi|internet|wireless', regex=True).astype(int)\n",
        "\n",
        "    # 8. Enhanced price handling with currency validation\n",
        "    df['price'] = (\n",
        "        df['price']\n",
        "        .replace('[^0-9.]', '', regex=True)  # Remove non-numeric characters\n",
        "        .astype(float)\n",
        "    )\n",
        "    # Handle missing prices using city-level median\n",
        "    df['price'] = df.groupby('city')['price'].transform(\n",
        "        lambda x: x.fillna(x.median())\n",
        "    )\n",
        "    # Outlier handling using city-specific ranges\n",
        "    def city_clipper(group):\n",
        "        q1 = group.quantile(0.05)\n",
        "        q3 = group.quantile(0.95)\n",
        "        return group.clip(q1, q3)\n",
        "    df['price'] = df.groupby('city')['price'].transform(city_clipper)\n",
        "\n",
        "    # 9. Enhanced review handling\n",
        "    df['has_review'] = df['review_scores_rating'].notna().astype(int)\n",
        "    df['review_scores_rating'] = df['review_scores_rating'].fillna(0)\n",
        "\n",
        "    # 10. Instant bookable with validation\n",
        "    df['instant_bookable'] = df['instant_bookable'].map({'t': 1, 'f': 0, None: 0}).fillna(0)\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    df = df.drop(columns=['amenities', 'description', 'bathrooms_text'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply cleaning to listings\n",
        "cleaned_listings = clean_listings(all_listings)"
      ],
      "metadata": {
        "id": "Ko_gZKTsys3v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# 2. Data Cleaning for Calendars (Enhanced)\n",
        "# ==============================================\n",
        "def clean_calendars(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean and engineer features for the calendar dataset.\"\"\"\n",
        "    # Convert date column\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "    # 1. Enhanced availability conversion\n",
        "    df['occupied'] = (\n",
        "        df['available']\n",
        "        .replace({'t': 0, 'f': 1, '': np.nan, None: np.nan})\n",
        "        .astype(float)\n",
        "    )\n",
        "    df['occupied'] = df['occupied'].fillna(0).astype(int)\n",
        "\n",
        "    # 2. Seasonal features with proper date handling\n",
        "    df['month'] = df['date'].dt.month\n",
        "    seasons = [\n",
        "        (df['month'].isin([12, 1, 2]), 'Winter'),\n",
        "        (df['month'].isin([3, 4, 5]), 'Spring'),\n",
        "        (df['month'].isin([6, 7, 8]), 'Summer'),\n",
        "        (df['month'].isin([9, 10, 11]), 'Fall')\n",
        "    ]\n",
        "    df['season'] = np.select([cond for cond, _ in seasons], [label for _, label in seasons])\n",
        "\n",
        "    # 3. Weekend flag with edge case handling\n",
        "    df['is_weekend'] = (df['date'].dt.dayofweek >= 5).astype(int)\n",
        "\n",
        "    # 4. Enhanced Canadian holiday handling\n",
        "    ca_holidays = holidays.CA(years=2024)\n",
        "    additional_holidays = {\n",
        "        '2024-04-01': \"Easter Monday (Observed)\",\n",
        "        '2024-05-20': \"Victoria Day\",\n",
        "        '2024-07-01': \"Canada Day\",\n",
        "        '2024-09-02': \"Labour Day\",\n",
        "        '2024-10-14': \"Thanksgiving\",\n",
        "        '2024-12-25': \"Christmas Day\",\n",
        "        '2024-12-26': \"Boxing Day\"\n",
        "    }\n",
        "    df['is_holiday'] = (\n",
        "        df['date'].astype(str).isin(additional_holidays.keys())\n",
        "        | df['date'].isin(ca_holidays)\n",
        "    ).astype(int)\n",
        "    false_positives = ['2024-03-29']\n",
        "    df['is_holiday'] = np.where(\n",
        "        df['date'].astype(str).isin(false_positives),\n",
        "        0,\n",
        "        df['is_holiday']\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply cleaning to calendars\n",
        "cleaned_calendars = clean_calendars(all_calendars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNUo1w7Ly2jU",
        "outputId": "ba6c1944-b439-427b-d44e-389b8be0d5a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-b10cb90862d4>:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  .replace({'t': 0, 'f': 1, '': np.nan, None: np.nan})\n",
            "<ipython-input-3-b10cb90862d4>:43: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
            "  | df['date'].isin(ca_holidays)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# 3. Calculate occupancy_365 and import to listings file\n",
        "# ==============================================\n",
        "occupancy_365 = (\n",
        "    cleaned_calendars.groupby('listing_id')['occupied']\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .rename(columns={'occupied': 'occupancy_365'})\n",
        ")\n",
        "\n",
        "cleaned_listings = pd.merge(\n",
        "    cleaned_listings,\n",
        "    occupancy_365,\n",
        "    left_on='id',\n",
        "    right_on='listing_id',\n",
        "    how='left'\n",
        ").drop(columns=['listing_id'])\n",
        "\n",
        "# Fill NA with 0 (if the listing has no occupancy data)\n",
        "cleaned_listings['occupancy_365'] = cleaned_listings['occupancy_365'].fillna(0)"
      ],
      "metadata": {
        "id": "sFhu8-6Fy8ot"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==============================================\n",
        "# 4. Save Cleaned Data\n",
        "# ==============================================\n",
        "cleaned_listings.to_csv(\"cleaned_listings_v2.csv\", index=False)\n",
        "cleaned_calendars.to_csv(\"cleaned_calendars_v2.csv\", index=False)\n",
        "\n",
        "print(\"Data cleaning complete!\")\n",
        "print(f\"Listings shape: {cleaned_listings.shape}\")\n",
        "print(f\"Calendars shape: {cleaned_calendars.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di4C5fkjGo6Z",
        "outputId": "8081cc77-e0e6-45a2-a934-b246c59ee0a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaning complete!\n",
            "Listings shape: (34734, 23)\n",
            "Calendars shape: (12712644, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create daily metrics for visualization\n",
        "daily_metrics = cleaned_calendars.groupby(['city', 'date']).agg(\n",
        "    total_listings=('listing_id', 'nunique'),\n",
        "    occupied=('occupied', 'sum')\n",
        ").reset_index()\n",
        "daily_metrics['occupancy_rate'] = daily_metrics['occupied'] / daily_metrics['total_listings']\n",
        "daily_metrics.to_csv(\"daily_metrics.csv\", index=False)\n",
        "print(f\"Daily Metrics shape: {daily_metrics.shape}\")"
      ],
      "metadata": {
        "id": "rfrlj67xCjTK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}